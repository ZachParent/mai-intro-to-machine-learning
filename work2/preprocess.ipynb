{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import seaborn\n",
    "from scipy.io import arff\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions relating to loading data\n",
    "\n",
    "# Function to return full project path\n",
    "def get_full_path():\n",
    "    return os.getcwd()\n",
    "\n",
    "# Function to load ARFF files and return a pandas DataFrame\n",
    "def load_arff_file(file_path):\n",
    "    data, meta = arff.loadarff(file_path)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Function to load all ARFF files in a directory\n",
    "def load_all_arff_files(directory):\n",
    "    # List to hold all data relating to a dataset\n",
    "    all_data = [] \n",
    "    \n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".arff\"): \n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            # Load the arff file\n",
    "            df = load_arff_file(file_path)\n",
    "\n",
    "            # Add the resulting df to the list\n",
    "            all_data.append(df) \n",
    "    \n",
    "    # Return the list of DataFrames for further use\n",
    "    return all_data\n",
    "\n",
    "def combine_arff_files(dataset):\n",
    "    return pd.concat(dataset, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rows with missing values: 46.45%\n",
      "Percentage of rows with missing values: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Function to compute the percentage of rows with missing values\n",
    "def percentage_missing_values(df):\n",
    "    # Total rows\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Rows with any missing values (returns a boolean Series)\n",
    "    missing_rows_mask = df.isnull().any(axis=1)\n",
    "    \n",
    "    # Number of rows with missing values\n",
    "    missing_rows_count = missing_rows_mask.sum()\n",
    "    \n",
    "    # Percentage of rows with missing values\n",
    "    percentage = (missing_rows_count / total_rows) * 100\n",
    "    \n",
    "    print(f\"Percentage of rows with missing values: {percentage:.2f}%\")\n",
    "    return percentage\n",
    "\n",
    "proj_path = get_full_path()\n",
    "\n",
    "dataset_one_dir = f\"{proj_path}\\\\datasetsCBR\\\\hepatitis\" # Change end of path to whichever 2 datasets we end up choosing\n",
    "dataset_two_dir = f\"{proj_path}\\\\datasetsCBR\\\\mushroom\" # Change end of path to whichever 2 datasets we end up choosing\n",
    "\n",
    "# Load all of the arff files associated with a dataset\n",
    "arff_files_data_one = load_all_arff_files(dataset_one_dir)\n",
    "arff_files_data_two = load_all_arff_files(dataset_two_dir)\n",
    "\n",
    "# Combine all of the arff files into 1 dataset\n",
    "combined_dataset_one = combine_arff_files(arff_files_data_one)\n",
    "combined_dataset_two = combine_arff_files(arff_files_data_two)\n",
    "\n",
    "# Calculate the percentage of rows with missing values\n",
    "missing_values_one = percentage_missing_values(combined_dataset_one)\n",
    "missing_values_two = percentage_missing_values(combined_dataset_two)\n",
    "\n",
    "#print(f\"Percentage of rows that have missing values in 'adult' dataset: {missing_values_one}\")\n",
    "#print(f\"Percentage of rows that have missing values in 'mushroom' dataset: {missing_values_two}\")\n",
    "\n",
    "# Things to consider:\n",
    "# - Need a definitive set of datasets to know how to pre-process the data\n",
    "# - Need to define what is considered null value (e.g.: NaN?)\n",
    "# - What to do with missing values\n",
    "# - Normalisation (need actual datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
