\subsection{Support Vector Machines (SVM)}
\label{subsec:methods-svm}

This section describes the Support Vector Machines (SVM) algorithm and its implementation in our study.

\subsubsection{Algorithm Overview}

Support Vector Machines (SVM) is a powerful supervised learning algorithm used for classification and
regression tasks. The primary objective of SVM is to find the optimal hyperplane that separates different classes
in the feature space while maximizing the margin between the classes\cite{burges1998svmTutorial}.

The key principles of SVM include:

\begin{itemize}
    \item Margin Maximization: SVM aims to find the hyperplane that maximizes the margin between classes, which enhances the model's generalization capability.
    \item Support Vectors: The data points closest to the decision boundary, known as support vectors, play a crucial role in defining the optimal hyperplane.
    \item Kernel Trick: SVM can handle non-linearly separable data by mapping the input space to a higher-dimensional feature space using kernel functions.
\end{itemize}

Advantages of SVM include:
\begin{itemize}
    \item Effectiveness in high-dimensional spaces
    \item Versatility through different kernel functions
    \item Faster consultation times than KNN, thanks to training step
\end{itemize}

Disadvantages of SVM include:
\begin{itemize}
    \item Sensitivity to the choice of kernel function and hyperparameters
    \item Computational complexity for large datasets
\end{itemize}
\subsubsection{Implementation Details}

Unlike for KNN (see \autoref{subsec:methods-knn} on K-Nearest Neighbors), we did not implement the SVM algorithm ourselves.
Instead, we used the prebuilt implementations from the \texttt{sklearn} library
\footnote{Scikit-learn's \texttt{svm} module documentation can be found at \url{https://scikit-learn.org/1.5/modules/svm.html}}.
Scikit-learn's \texttt{svm} module includes several different implementations of SVM:

\begin{itemize}
    \item \texttt{SVC}: Support Vector Classification, the most commonly used implementation for classification tasks
    \item \texttt{NuSVC}: Support Vector Classification with Nu-SVC, similar to \texttt{SVC} but with a different formulation of the optimization problem 
    \item \texttt{LinearSVC}: Linear Support Vector Classification, a specific variant of \texttt{SVC} that uses a linear kernel
    \item \texttt{SVR}: Support Vector Regression, a variant of SVM for regression tasks
\end{itemize}

For our study, we decided to use \texttt{SVC} as it is the most commonly used implementation
for classification tasks, allowed for the kernel trick (unlike \texttt{LinearSVC}), and met our needs.

\subsubsection{Kernel Selection}

In our study, we explored multiple kernel functions to capture different types of relationships in the data. The kernels used include:

\begin{itemize}
    \item Linear: $K(x_i, x_j) = x_i^T x_j$
    \item Polynomial: $K(x_i, x_j) = (\gamma x_i^T x_j + r)^d$
    \item Radial Basis Function (RBF): $K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)$
    \item Sigmoid: $K(x_i, x_j) = \tanh(\gamma x_i^T x_j + r)$
\end{itemize}

Where $x_i$ and $x_j$ are feature vectors, $\gamma$ is a kernel coefficient, $r$ is a constant term, and $d$ is the degree of the polynomial kernel.

\subsubsection{Hyperparameter Tuning}

The following hyperparameters were tuned in our SVM implementation:

\begin{itemize}
    \item C: The regularization parameter, which controls the trade-off between achieving a low training error and a low testing error. We explored values [1, 3, 5, 7].
    \item Kernel: We tested different kernel types including "linear", "poly", "rbf", and "sigmoid".
\end{itemize}

\subsubsection{Implementation Steps}

The SVM classification process in our study followed these steps:

\begin{enumerate}
    \item Data Preparation (see \autoref{sec:data} on Data).
    \item Model Configuration: SVM models were created with different combinations of C values and kernel types.
    \item Cross-validation: For each configuration, the model was trained and evaluated using cross-validation across 10 predefined folds.
    \item Performance Evaluation: Various metrics including accuracy, F1 score, and confusion matrix elements (TP, TN, FP, FN) were computed.
    \item Time Measurement: Training and testing times were recorded for each configuration.
    \item Results Compilation: The results for each configuration were saved in CSV files for further analysis.
\end{enumerate}

\subsubsection{Multi-class Classification}

While simple SVMs are binary classifiers, they can be extended to multi-class classification through various strategies.
In our case, however, both the datasets included only two classes, so we did not need to use these strategies.

\subsubsection{Performance Metrics}

The performance of the SVM models was evaluated using the following metrics:

\begin{itemize}
    \item Accuracy: The proportion of correct predictions among the total number of cases examined.
    \item F1 Score: The harmonic mean of precision and recall, providing a balanced measure of the model's performance.
    \item Confusion Matrix: Including True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).
    \item Training Time: The time taken to train the model.
    \item Testing Time: The time taken to make predictions on the test set.
\end{itemize}

The F1 score was used as our primary metric for evaluating the performance of the SVM models,
since it balances the trade-off between precision and recall, and does not report overly favorable results when the dataset is imbalanced.

