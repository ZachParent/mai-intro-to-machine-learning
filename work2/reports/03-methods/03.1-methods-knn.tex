\subsection{k-Nearest Neighbors (kNN)}
\label{subsec:methods-knn}

This section describes the k-Nearest Neighbors (kNN) algorithm and its implementation in our study.

\subsubsection{Algorithm Overview}

The k-NN algorithm operates on a simple yet effective principle: when classifying new data points, 
it examines the k closest training examples and assigns the most common class among these neighbors (where k is a user-defined hyperparameter). 
The algorithm's effectiveness relies on two fundamental assumptions:
\begin{itemize}
    \item Locality: Points that are close to each other are likely to have the same class.
    \item Smoothness: The classification boundary between classes is relatively smooth.
\end{itemize}

One key feature of k-NN is it employs neighbor-based classification, where the classification of a new data point 
is determined by majority voting among its k-nearest neighbors. 
The value of k is one of the most important tunable hyperparameters, as it significantly influences the algorithm's behaviour:
\begin{itemize}
    \item Small k values (e.g., k=1 or k=3): More sensitive to local patterns but susceptible to noise.
    \item Large k values: More robust to noise but may overlook important local patterns.
    \item Even vs. Odd k values: Even k values result in ties, which may require additional rules to break.
\end{itemize}

The dependent variable in our datasets we aim to predict is categorical, so while k-NN can be used for both classification and regression problems 
our focus is on classification.

While the k-NN algorithm has it's merits in terms of simplicity and interpretability, it also has several disadvantages:
\begin{itemize}
    \item Computationally expensive: As the number of training examples grows, the algorithm's complexity increases[1]. 
    \item Sensitive to irrelevant features: The algorithm treats all features equally, so irrelevant features can negatively impact performance.
    \item Curse of dimensionality: As the number of features increases, the algorithm requires more data to maintain performance.
\end{itemize}

All three of the above mentioned disadvantages all relate to the features of the dataset, and how they can impact the performance of the algorithm.
They create a compounding effect: more features lead to higher computational cost, while making the algorithm more susceptible to noise and irrelevant features,
and also requiring more data to maintain performance. This is why the use of feature selection and reduction techniques are imperative 
when working with the k-NN algorithm to increase performance.


\subsubsection{Implementation Details}
The K-Nearest Neighbors (k-NN) algorithm is implemented using Python with various libraries and tools.
Below are the specific implementation details:

\subsubsection*{Distance calculations}
The Euclidean distance is used to measure the distance between data points. 
This distance metric is the most commonly used distance metric in k-NN algorithms
due to its simplicity and effectiveness in measuring the similarity between data points[2].
It operates on the principle of calculating the straight-line distance between two points in a Euclidean space, hence it's simplicity.

The formula for Euclidean distance between two vectors \(x\) and \(y\) is:
\[ d = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2} \]
where \(x_i\) and \(y_i\) are the \(i\)th elements of vectors \(x\) and \(y\), respectively.\\

The Manhattan distance is another distance metric that can be used in k-NN algorithms. It is also frequently used with the k-NN algorithm,
albeit not as common as the Euclidean distance.
The Manhattan distance is calculated by summing the absolute differences between the coordinates of two points.

The formula for Manhattan distance between two vectors \(x\) and \(y\) is:
\[ d = \sum_{i=1}^{n} |x_i - y_i| \]
where \(x_i\) and \(y_i\) are the \(i\)th elements of vectors \(x\) and \(y\), respectively.\\

The Chebychev distance less commonly used than the Euclidean and Manhattan distances
in relation to the k-NN algorithm, but it is still a valid distance metric and operates effectively in measuring the similarity between data points.
The Chebychev distance is calculated by taking the maximum absolute difference between the coordinates of two points. This differs from Manhattan distance
in that it takes the maximum absolute difference, rather than the sum.

The formula for Chebychev distance between two vectors \(x\) and \(y\) is:
\[ d = \max_{i=1}^{n} |x_i - y_i| \]
where \(x_i\) and \(y_i\) are the \(i\)th elements of vectors \(x\) and \(y\), respectively.\\

Along with the Chebychev distance, the Malahanobis distance is another distance metric used in k-NN algorithms.
It is also less commonly used than the Euclidean and Manhattan distance metrics.
The Mahalanobis distance is calculated by taking the square root of the sum of the squared differences between the coordinates of two points,
where the squared differences are divided by the covariance matrix of the data.

The formula for Mahalanobis distance between two vectors \(x\) and \(y\) is:
\[ d = \sqrt{(x - y)^T \cdot S^{-1} \cdot (x - y)} \]
where \(x\) and \(y\) are the vectors, and \(S\) is the covariance matrix of the data.\\

\subsubsection*{Weighting Schemes}
In the k-NN algorithm, the choice of weighting scheme can significantly impact the classification results.
The following weighting schemes were implemented in this study:\\

In uniform weighting, all neighbours have equal weight in the voting process.
This is the default weighting scheme in k-NN.
An advantage of uniform weighting is that it is simple and computationally efficient, but it may not be optimal for imbalanced datasets.\\

With ReliefF weighting, neighbours are weighted based on their relevance to the target class.
This provides a more nuanced approach to weighting as it considers the importance of each neighbour in the classification process, potentially
improving performance of the algorithm.\\

Information gain weighting, Neighbours are weighted based on gain in information in the context of the target variable [3].
Similarly to ReliefF weighting, this weighting scheme assigns higher weights to neighbours that provide more information about the target class.\\

\subsubsection*{Voting Schemes}
In the k-NN algorithm, the voting scheme determines how the class label of a new data point is determined based on the class labels of its k-nearest neighbors.
The following voting schemes were implemented in this study:\\

In the majority voting scheme, the class label with the highest frequency among the k-nearest neighbors is assigned to the new data point.
As well as this, each vote is given equal weight in the voting process [4].
This is the default voting scheme in k-NN and is simple and easy to implement.\\

With inverse distance weighting voting, the class labels of the k-nearest neighbors are weighted based on their distance from the new data point.
While there are different methods to perform this weighting, the most simple version is to take a neighbour's vote to be the inverse
of its distance to q:

\[ w_i = \frac{1}{d(q, x_i)} \]

where \(w_i\) is the weight of the \(i\)th neighbour, \(d(q, x_i)\) is the distance between the new data point \(q\) and the \(i\)th neighbour \(x_i\).

Then the votes are summed and the class with the highest votes is returned [4].\\

Shepard's method is another voting scheme that can be used in k-NN algorithms. 
It employs the use of an exponential function to weight the votes of the neighbours based on their distance from the new data point,
rather than the inverse of the distance [5].

The formula for Shepard's voting scheme is:
\begin{equation} 
    Vote(y_j)=\sum_{i=1}^k e^{-d(\mathbf{q,x}_i)^2}1(y_j,y_c)
\end{equation}

where \(Vote(y_j)\) is the vote for class \(y_j\), \(d(\mathbf{q,x}_i)\) is the
distance between the new data point \(\mathbf{q}\) and the \(i\)th neighbour \(\mathbf{x}_i\),
and \(1(y_j,y_c)\) is the indicator function that returns 1 if \(y_j\) is the same as
the class label \(y_c\) of the \(i\)th neighbour, and 0 otherwise.\\

\subsubsection*{Neighbor Selection}
The choice of the number of neighbors (k) is a critical hyperparameter in the k-NN algorithm, as previously discussed in this report.
Different values of k can significantly impact the algorithm's performance,
with smaller values being more sensitive to noise and larger values potentially overlooking important local patterns.
It's imperative to choose an optimal value of k that balances these trade-offs and maximizes the algorithm's performance.
In this study, the following values of k where examined: [1, 3, 5, 7] \\

\subsubsection*{Basic Algorithm Steps}
The k-NN algorithm can be summarized in the following steps:
\begin{enumerate}
    \item Preprocess the data:
        \begin{itemize}
            \item Scale/normalize features to ensure equal contribution
            \item Handle missing values
            \item Encode categorical variables if necessary
        \end{itemize}
    \item Optimize the model:
        \begin{itemize}
            \item Tune hyperparameters (k, distance metric, weighting scheme, voting scheme)
            \item Consider dimensionality reduction techniques
            \item Implement feature selection if necessary
            \item Balance dataset if required
        \end{itemize}
    \item For each query point \(\mathbf{q}\):
        \begin{itemize}
            \item Calculate distances \(d(\mathbf{q,x}_i)\) to all training examples
            \item Sort distances to identify the k-nearest neighbors
            \item Apply selected weighting scheme to neighbour votes
            \item Determine class label using chosen voting method
        \end{itemize}
    \item Validate the model:
        \begin{itemize}
            \item Split data into training and validation sets
            \item Evaluate using appropriate metrics (accuracy, precision, recall, F1)
            \item Perform cross-validation to assess generalization
        \end{itemize}
\end{enumerate}

% Check if more are to be added
\subsubsection*{Libraries and Tools}
The following libraries and tools were used for the implementation of our study:
\begin{itemize}
    \item \textbf{Python}: The primary programming language used for the implementation of the k-NN algorithm.
    \item \textbf{NumPy}: A useful package for scientific computing with Python, used for numerical operations.
    \item \textbf{Scikit-learn}: A machine learning library in Python, used for implementing the k-NN algorithm.
    \item \textbf{Pandas}: A data manipulation library in Python, used for data preprocessing and analysis.
    \item \textbf{Matplotlib}: A plotting library in Python, used for data visualization.
    \item \textbf{Seaborn}: A data visualization library in Python, used for creating informative and attractive statistical graphics.
    \item \textbf{Jupyter Notebook}: An interactive development environment used for running Python code and visualizing results.
    \item \textbf{SciPy}: A scientific computing library in Python, used for scientific and technical computing.
    \item \textbf{TensorFlow / PyTorch}: An open-source machine learning library in Python, used for building and training machine learning models.
\end{itemize}

\subsubsection{Parameter Tuning}
The k-NN algorithm's performance depends significantly on the careful tuning of several key parameters.
In this study, we employed a systematic approach to parameter optimization using manual grid search with
cross-validation. The following parameters were tuned:

\subsubsection*{Data Preprocessing}
Before parameter tuning, comprehensive preprocessing pipelines were implemented for both the hepatitis
and mushroom datasets using scikit-learn's ColumnTransformer and Pipeline classes.
The preprocessing steps were customized for each dataset's specific characteristics:

\paragraph{Hepatitis Dataset Preprocessing}
The hepatitis dataset required handling of both numeric and categorical features:
\begin{itemize}
    \item \textbf{Numeric Features}: The following features were processed using mean imputation and Min-Max scaling:
    \begin{itemize}
        \item AGE
        \item ALK\_PHOSPHATE
        \item SGOT
        \item BILIRUBIN
        \item ALBUMIN
        \item PROTIME
    \end{itemize}
    \item \textbf{Categorical Features}: The following features were processed using mode imputation and label encoding:
    \begin{itemize}
        \item SEX
        \item STEROID
        \item ANTIVIRALS
        \item FATIGUE
        \item MALAISE
        \item ANOREXIA
        \item Other binary indicators (LIVER\_BIG, LIVER\_FIRM, etc.)
    \end{itemize}
\end{itemize}

\paragraph{Mushroom Dataset Preprocessing}
The mushroom dataset consisted entirely of categorical features:
\begin{itemize}
\item \textbf{22 Categorical Features}: Including:
    \begin{itemize}
    \item cap-shape
    \item cap-surface
    \item cap-color
    \item bruises?
    \item odor
    \item gill-related features
    \item stalk-related features
    \item Other morphological characteristics
    \end{itemize}
\end{itemize}

\begin{verbatim}
    preprocessor = ColumnTransformer(
    transformers=[
    ('num', Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', MinMaxScaler())
    ]), numeric_cols),
    ('cat', Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('passthrough', 'passthrough')
    ]), categorical_cols)
    ]
    )
\end{verbatim}

\paragraph{Missing Value Handling}
    \begin{itemize}
    \item Question marks (`?') were replaced with \texttt{np.nan}
    \item Numeric features: Missing values imputed using mean strategy
    \item Categorical features: Missing values imputed using mode strategy
    \end{itemize}

\paragraph{Feature Encoding}
Label encoding was applied to all categorical features:
\begin{verbatim}
    for col in categorical_cols:
    le = LabelEncoder()
    processed_df[col] = le.fit_transform(processed_df[col])
\end{verbatim}

%TODO: Verify this
\subsubsection*{K Value Selection}
The optimal value of k was determined through k-fold cross-validation (k=10) across the candidate k values [1, 3, 5, 7].
For each dataset, we evaluated the performance metrics (accuracy, precision, recall, and F1-score) across these k values.
To avoid ties in classification, we primarily focused on odd values of k.
The final k value was selected based on the best average performance across all folds, also taking into account
the different hyperparameters and their impact on the model's performance.

\subsubsection*{Distance Metric Optimization}
We evaluated four distance metrics:
\begin{itemize}
\item Euclidean distance
\item Manhattan distance
\item Chebyshev distance
\item Mahalanobis distance
\end{itemize}

Each distance metric was tested in combination with different k values to identify the optimal pairing.
The Mahalanobis distance required additional computation of the covariance matrix

\subsubsection*{Weighting Scheme Selection}
Three weighting schemes were evaluated through cross-validation:
\begin{itemize}
\item Uniform weighting (baseline)
\item ReliefF weighting
\item Information gain weighting
\end{itemize}

The optimal weighting scheme was selected based on both performance metrics and computational efficiency considerations.

\subsubsection*{Voting Scheme Optimization}
We compared three voting schemes:
\begin{itemize}
\item Majority voting
\item Inverse distance weighting
\item Shepard's method
\end{itemize}

Each voting scheme was evaluated across different combinations of k values and distance metrics

\subsubsection*{Parameter Search Implementation}
The optimal combination of parameters was determined using a custom grid search function that evaluated all possible combinations of:
\begin{itemize}
    \item k values
    \item Distance functions
    \item Voting schemes
    \item Weighting schemes
\end{itemize}

The final model was selected based on the best average performance across all folds in the cross-validation process.

% References
% [1] LaViale, T. (2023). Deep Dive on KNN: Understanding and Implementing the K-Nearest Neighbors Algorithm. [online] Arize AI. Available at: https://arize.com/blog-course/knn-algorithm-k-nearest-neighbor/.
% [2] IBM (2023). What Is the k-nearest Neighbors algorithm? | IBM. [online] www.ibm.com. Available at: https://www.ibm.com/topics/knn.
% [3] Brownlee, J. (2019). Information Gain and Mutual Information for Machine Learning. [online] Machine Learning Mastery. Available at: https://machinelearningmastery.com/information-gain-and-mutual-information/.
% [4] Classification: k Nearest Neighbours. (n.d.). Available at: https://www.cs.ucc.ie/~dgb/courses/tai/notes/handout4.pdf [Accessed 26 Oct. 2024].
% [5] Padraig Cunningham, Sarah Jane Delany, k-Nearest Neighbour Classifiers - A Tutorial. ACM Comput. Surv. 54(6): 128:1-128:25 (2021), DOI: 10.1145/3459665
‌
