\subsection{Statistical Analysis}
\label{sec:statistical-analysis}

This section presents the statistical analysis of the results of the study.
We employ a systematic approach to compare the performance of different models and their configurations.

\subsubsection*{Tests Considered}
There are various statistical tests available to compare the performance of machine learning models.
We considered the following tests for our analysis:

% TODO: Citation needed
\subsubsection{ANOVA}
Analysis of Variance (ANOVA) decomposes the total variance in the data into different components such as:
between-classifier variability, between-dataset variability, and residual variability. When between-classifier
variability is significantly larger than the residual variability, we can reject the theorised null hypothesis and
conclude that there is a significant difference between the classifiers.

However, ANOVA assumes that the data is drawn from normal distributions and that the variances are equal across groups.
In the case of the hepatitis dataset, the class distribution is skewed (79 $-$ 21 split) [CITE].

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/hepatitis-class-distribution.png}
    \caption{Class distribution of the Hepatitis dataset}
\label{fig:class-distribution-hepatitis}
\end{figure}

Furthermore, the performance metrics distribution across folds in Figure~\ref{fig:ranked-folds-knn-hepatitis} further demonstrates non-normal distribution:

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/ranked_folds_KNN_hepatitis.png}
    \caption{Rank distributions across folds for different KNN configurations on the Hepatitis dataset}
\label{fig:ranked-folds-knn-hepatitis}
\end{figure}

These plots verify that the data does not meet the assumptions of ANOVA:

- Non-normal distribution: Several models show asymmetric distributions, where the median line is not in the middle of the box.
Also present are outliers in the data, which are represented as dots outside of the whisker lines.
- Unequal variances: Box sizes vary significantly across models, and some models have much larger spreads than others.
Additionally, the whisker lenghts vary considerably between models.

These claims hold true when analysing performance metrics distribution across folds using SVM:

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/ranked_folds_SVM_hepatitis.png}
    \caption{Rank distributions across folds for different KNN configurations on the Hepatitis dataset}
\label{fig:ranked-folds-svm-hepatitis}
\end{figure}

Given these observations, we decided against using ANOVA for our analysis.

% TODO: Citation needed
\subsubsection{Friedman Test}
The Friedman test is a non-parametric equivalent of the repeated-measures ANOVA. 
For each dataset, it ranks the models separately with the best model receiving a rank of 1, the second-best a rank of 2, and so on.
The average rank is used to settle any ties in the ranks [CITE].

While the Friedman test theoretically has less statistical power than ANOVA when ANOVA's assumptions are met,
it is more suitable for our analysis as it makes no assumptions about normality or equal variances.
As we demonstrated with the hepatitis dataset's class distribution and performance metrics, these assumptions do not hold in our case.

\subsubsection{Linear vs. Top Sampling Justification}
The results of the Nemenyi test must satisfy the requirements to perform post-hoc testing.
If the Nemenyi test fails, we cannot proceed with post-hoc testing as the results would be considered unreliable.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/p_values_vs_num_samples_mushroom.png}
    \includegraphics[width=0.8\textwidth]{figures/p_values_vs_num_samples_hepatitis.png}
    \caption{P values vs. number of samples for different models and sampling methods for the datasets}
\label{fig:p-values-vs-num-samples}
\end{figure}

As seen in the plots in Figure~\ref{fig:p-values-vs-num-samples}, we perform linear sampling and top sampling with a variety
of sample sizes. With top sampling, we sample the top models based on their f1 score, while linear sampling samples models
evenly across the performance spectrum. The results show that the Nemenyi test fails across the various sample sizes for both datasets
when using top sampling but passes when using linear sampling. This indicates that the top sampling method is not suitable for our analysis
as we cannot perform post-hoc testing on the models. Therefore, we must proceed with linear sampling for our analysis.

% TODO: Citation needed
\subsubsection{Post-Hoc Tests}
The Friedman test only tells us that there is a significant difference between the models,
but it does not tell us which models are significantly different from each other.
This is why we employ the Nemenyi test as a post-hoc procedure.

The Nemenyi test compares the performance of all classifiers to each
other by checking if their average ranks differ by at least the critical difference:

\begin{equation}
    CD = q_{\alpha} \times \sqrt{\frac{k(k+1)}{6N}}
\end{equation}

where $q_{\alpha}$ is based on the Studentized range statistic, $k$ is the number of classifiers, and $N$ is the number of datasets [CITE].
If the difference in average ranks between two models exceeds this critical difference, we can conclude that their performances are significantly different.

When performing the Nemenyi test, proper handling of ties is crucial for accurate analysis.
Instead of arbitrarily assigning consecutive ranks to tied values (e.g., ranks 2 and 3), we use the
average of the ranks they would have occupied. For example, if two models tie for second place,
rather than arbitrarily assigning ranks 2 and 3, both models receive a rank of 2.5 (the average of positions 2 and 3).
This mid-rank approach ensures fair treatment of tied performances and prevents artificial rank differences that could bias the statistical analysis.

\subsubsection{Test Results}
In the visualization of critical difference (CD) diagrams,the entire bar represents the critical difference value.
The half-width of the bar (CD/2) extends on either side of a model's average rank. When comparing two models,
if their CD/2 intervals do not overlap, we can conclude that there is a statistically significant difference in their performance.
This visual representation provides an intuitive way to interpret the Nemenyi test results, as any
non-overlapping bars clearly indicate significant differences between models.

For the below analysis of both the k-NN and SVM models, values closer to 1 indicate no significant difference between models,
whereas values closer to 0 indicate a significant difference.

The diagonal values (top left to bottom right) are always 1 and report no significant difference
as they represent the comparison of a model with itself.

To determine statistical significance, we use a confidence level of 0.95 (95\% confidence interval) for the Nemenyi test.
Therefore, any values greater than 0.05 indicate that the models are not significantly different from each other.

\subsubsection*{k-Nearest Neighbors (KNN) Analysis}
Below are the Nemenyi test results for the KNN models on the hepatitis and mushroom datasets respectively.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/nemenyi_test_results_KNN_hepatitis.png}
    \includegraphics[width=0.8\textwidth]{figures/nemenyi_test_results_KNN_mushroom.png}
    \caption{Nemenyi test results for KNN models}
\label{fig:nemenyi-knn}
\end{figure}

The Nemenyi test results for the KNN models on the hepatitis dataset reveal two statistically distinct
groups of model configurations. The first group consists of k = 1 configurations
(with Manhattan and Euclidean distance) and k=3 with Manhattan distance, which all perform similarly to each other (p=1.00).
The second group includes configurations with higher k values (3,5,7) using various distance metrics,
which also perform similarly within their group (p=1.00) but significantly differently from the first
group (some comparisons yield p=0.04, below the 0.05 threshold). This clear separation suggests that the choice of k-value
has a more substantial impact on model performance than the choice of distance metric, with k=1 configurations behaving distinctly from higher k-values.

The Nemenyi test results for the KNN models on the mushroom dataset reveal two main groups of model configurations.
The first group consists of k=1 configurations and k=3 with Manhattan distance, which perform similarly
(p=0.85 $-$ 1.00), though with some variation within the group. The second group includes configurations
from k5ManShpInf through k7ChebShpRtf, which show high similarity within their group (p=0.95 $-$ 1.00).
Some of these groups perform significantly differently from each other, given the p-values fall below the 0.05 threshold.

We can deduce that the k-value has a substantial impact on model performance on both datasets. 

\subsubsection*{k-Nearest Neighbors (SVM) Analysis}
Below are the Nemenyi test results for the SVM models on the hepatitis and mushroom datasets respectively.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/nemenyi_test_results_SVM_hepatitis.png}
    \includegraphics[width=0.8\textwidth]{figures/nemenyi_test_results_SVM_mushroom.png}
    \caption{Nemenyi test results for SVM models}
\label{fig:nemenyi-svm}
\end{figure}

% TODO: Discuss the results for both datasets
