\subsubsection{k-Nearest Neighbors (KNN) Analysis}
\label{subsubsec:discussion-knn}

This section interprets the results of the KNN algorithm, discussing its performance and implications.

The KNN algorithm was evaluated using several performance metrics, including accuracy, precision, and recall and the f1 score.

\subsubsection*{General Performance Analysis}
As seen in Tables 1 and 2, the general performance of the KNN algorithm was excellent across both datasets,
achieving accuracy scores above 92\% and F1 scores above 92.5\% on all top ten configurations.

\subsubsection{Mushroom Dataset Performance}

\begin{description}
    \item[\textbf{Best Configuration:}]\leavevmode
        \begin{itemize}
            \item $k = 3, 5,$ or $7$ with Manhattan Distance (all performed equally well)
            \item Shepard's Work Voting scheme
            \item Equal Weighting
            \item \textbf{Results:} 95.1\% accuracy and 95.2\% F1 score
        \end{itemize}
    
    \item[\textbf{Performance Range:}]\leavevmode
        \begin{itemize}
            \item Accuracy: 92.6\% $--$ 95.1\%
            \item F1 Score: 92.7\% $--$ 95.2\%
        \end{itemize}
\end{description}

\subsubsection*{Observations}
Manhattan Distance consistently performed well across all configurations, with all top ten configurations using this distance metric.
The combination of Manhattan Distance and Shepard's Work Vote with a higher value of k was particularly effective. 
The higher performance of a larger k value suggests that the algorithm benefits from considering multiple neighbors when making predictions, perhaps
due to the dataset's inherent complexity and noise present in the data. The equal weighting scheme also performed excellently,
indicating that the dataset contains relatively a uniform
distribution of meaningful data points.
The F1 scores achieved across all configurations were consistently high, indicating a good balance between precision and recall.


\subsubsection{Hepatitis Dataset Performance}

\begin{description}
    \item[\textbf{Best Configuration:}]\leavevmode
        \begin{itemize}
            \item $k = 1$ with Euclidean Distance
            \item Shepard's Work Vote
            \item ReliefF Weighting
            \item \textbf{Results:} 95.5\% accuracy and 97.2\% F1 score
        \end{itemize}
    
    \item[\textbf{Performance Range:}]\leavevmode
        \begin{itemize}
            \item Accuracy: 94.8\% $--$ 95.5\%
            \item F1 Score: 96.7\% $--$ 97.2\%
        \end{itemize}
    
    \item[\textbf{Key Observations:}]\leavevmode
        \begin{itemize}
            \item Euclidean Distance performed best, particularly with ReliefF weighting.
            \item k = 1 configurations dominated the top results.
            \item All voting schemes performed similarly when other parameters were optimized.
        \end{itemize}
\end{description}

\subsubsection*{Observations}
The hepatitis dataset achieved its best performance with a k value of 1, indicating that the algorithm benefits from
more localized decisions when making predictions when using this dataset.
The Euclidean Distance metric performed best, particularly when combined with ReliefF weighting. This suggests that the dataset's features
have varying degrees of importance, and ReliefF weighting helps capture these distinctions effectively.
As also seen in the mushroom dataset, the F1 scores achieved across all configurations were consistently high,
indicating a good balance between precision and recall.


\subsubsection{Overall Comparison}
While both datasets achieved high performance, the hepatitis dataset outperformed the mushroom dataset in terms of accuracy and F1 score.
This difference may be due to the hepatitis dataset's smaller size and more distinct class separations,
making it easier for the KNN algorithm to make accurate predictions. These distinct class separations may also explain why the k = 1 configuration
performed best on the hepatitis dataset, as the algorithm can make more precise decisions with fewer neighbors.



