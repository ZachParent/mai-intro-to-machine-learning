\section{Introduction}
\label{sec:introduction}

Machine learning classification algorithms play a crucial role in modern data analysis, enabling automated decision-making across diverse domains from medical diagnostics to pattern recognition. This study focuses on evaluating and comparing two fundamental classification algorithms: k-Nearest Neighbors (KNN) and Support Vector Machines (SVM), along with instance reduction techniques to improve their efficiency.

\subsection{Background}

Classification algorithms assign predefined categories to data points based on their features. While many sophisticated classification methods exist, KNN and SVM remain widely used due to their interpretability and strong theoretical foundations. KNN operates by examining the closest training examples to make predictions, while SVM finds optimal hyperplanes to separate classes in feature space.

<<<<<<< HEAD
K-Nearest Neighbour (KNN) is a supervised machine learning algorithm that has been chosen as the primary method for this study.
It is a simple yet powerful algorithm that classifies new cases based on similarity to existing data points.
While KNN can be used for both classification and regression problems, our focus is on classification due to 
the nature of the datasets used \cite{knn}.\par
=======
A key challenge with instance-based methods like KNN is their computational and storage requirements, particularly for large datasets. Instance reduction techniques address this limitation by identifying and preserving only the most informative training examples, potentially improving both efficiency and generalization.
>>>>>>> 44b6abc19b24bcd22d2fd018bbbbae03100de9f4

\subsection{Research Objectives}

This study addresses three primary research questions:

\begin{enumerate}
    \item How do KNN and SVM compare in terms of classification accuracy and computational efficiency across datasets with different characteristics?
    \item What impact do various distance metrics, weighting schemes, and kernel functions have on model performance?
    \item Can instance reduction techniques maintain classification accuracy while significantly reducing storage and computational requirements?
\end{enumerate}

\subsection{Methodology Overview}

We evaluate these questions using two carefully selected datasets:
\begin{itemize}
    \item The Mushroom dataset (8,124 instances) - featuring categorical attributes and balanced classes
    \item The Hepatitis dataset (155 instances) - containing mixed data types and imbalanced classes
\end{itemize}

Our analysis employs rigorous statistical testing to compare model configurations and reduction techniques. We use cross-validation and multiple performance metrics to ensure robust conclusions.

\subsection{Contributions}

The key contributions of this work include:
\begin{itemize}
    \item A comprehensive comparison of KNN and SVM performance across diverse dataset characteristics
    \item Empirical evaluation of three instance reduction techniques (GCNN, ENNTh, and DROP3)
    \item Statistical analysis framework for comparing classification algorithms and reduction methods
    \item Practical guidelines for choosing between KNN and SVM based on dataset properties
\end{itemize}

\subsection{Report Structure}

The remainder of this report is organized as follows:
\begin{itemize}
    \item Section 2 describes the datasets and preprocessing methodology
    \item Section 3 details the implementation of KNN, SVM, and reduction techniques
    \item Section 4 presents experimental results and statistical analysis
    \item Section 5 concludes with key findings and future research directions
\end{itemize}