@article{burges1998svmTutorial,
  title={A Tutorial on Support Vector Machines for Pattern Recognition},
  author={Burges, Christopher J.C.},
  journal={Data Mining and Knowledge Discovery},
  volume={2},
  pages={121--167},
  year={1998},
  publisher={Springer},
  doi={10.1023/A:1009715923555}
}

@article{Wilson2000,
  title = {Reduction Techniques for Instance-Based Learning Algorithms},
  author = {Wilson, D. Randall and Martinez, Tony R.},
  journal = {Machine Learning},
  volume = {38},
  number = {3},
  pages = {257--286},
  year = {2000},
  doi = {10.1023/A:1007626913721},
  url = {https://doi.org/10.1023/A:1007626913721}
}

@misc{laviale2023,
  author = {Trevor LaViale},
  title = {Deep Dive on KNN: Understanding and Implementing the K-Nearest Neighbors Algorithm},
  year = {2023},
  url = {https://arize.com/blog-course/knn-algorithm-k-nearest-neighbor/}
}

@misc{brownlee2019,
  author = {Jason Brownlee},
  title = {Information Gain and Mutual Information for Machine Learning},
  year = {2019},
  url = {https://machinelearningmastery.com/information-gain-and-mutual-information/}
}

@misc{IBM2023,
  author = {IBM},
  title = {What Is the k-nearest Neighbors algorithm},
  year = {2023},
  url = {https://www.ibm.com/topics/knn}
}

@misc{uccNotes,
  author = {University College Cork},
  title = {Lecture Notes - Classification},
  url = {https://www.cs.ucc.ie/~dgb/courses/tai/notes/handout4.pdf}
}

@article{Cunningham2021,
  author    = {Padraig Cunningham and Sarah Jane Delany},
  title     = {k-Nearest Neighbour Classifiers - A Tutorial},
  journal   = {ACM Computing Surveys},
  volume    = {54},
  number    = {6},
  pages     = {128:1--128:25},
  year      = {2021},
  doi       = {10.1145/3459665}
}


@article{largeScalekNN,
  author = {Song, Y. and Kong, X. and Zhang, C.},
  title = {A large-scale k-nearest neighbor classification algorithm based on neighbor relationship preservation},
  journal = {Wireless Communications and Mobile Computing},
  year = {2022},
  volume = {2022},
  pages = {1-11},
  doi = {10.1155/2022/7409171}
}

@article{StatisticalComparisonsOfClassifiersOverMultipleDataSetsJML,
  title={Statistical Comparisons of Classifiers over Multiple Data Sets},
  journal={Journal of Machine Learning Research},
  author={Demsar, Janez},
  year={2006},
  volume={7},
  pages={1--30}
}



@article{GCNN-JMLR,
  author  = {Fu Chang and Chin-Chin Lin and Chi-Jen Lu},
  title   = {Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies},
  journal = {Journal of Machine Learning Research},
  year    = {2006},
  volume  = {7},
  number  = {76},
  pages   = {2125--2148},
  url     = {http://jmlr.org/papers/v7/chang06a.html}
}

@InProceedings{ENNTH,
  author="V{\'a}zquez, Fernando
  and S{\'a}nchez, J. Salvador
  and Pla, Filiberto",
  editor="Marques, Jorge S.
  and P{\'e}rez de la Blanca, Nicol{\'a}s
  and Pina, Pedro",
  title="A Stochastic Approach to Wilson's Editing Algorithm",
  booktitle="Pattern Recognition and Image Analysis",
  year="2005",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="35--42",
  abstract="Two extensions of the original Wilson's editing method are introduced in this paper. These new algorithms are based on estimating probabilities from the k-nearest neighbor patterns of an instance, in order to obtain more compact edited sets while maintaining the classification rate. Several experiments with synthetic and real data sets are carried out to illustrate the behavior of the algorithms proposed here and compare their performance with that of other traditional techniques.",
  isbn="978-3-540-32238-2"
}

@article{Friedman_posthoc,
author = {Dulce G. Pereira, Anabela Afonso and Fátima Melo Medeiros},
title = {Overview of Friedman’s Test and Post-hoc Analysis},
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {44},
  number = {10},
  pages = {2636--2653},
  year = {2015},
  publisher = {Taylor \& Francis},
  doi = {10.1080/03610918.2014.931971},
  URL = { 
    https://doi.org/10.1080/03610918.2014.931971
  },
  eprint = {   
    https://doi.org/10.1080/03610918.2014.931971
  }

}

@article{Friedman_anova,
  author = {Donald W. Zimmerman and Bruno D. Zumbo},
  title = {Relative Power of the Wilcoxon Test, the Friedman Test, and Repeated-Measures ANOVA on Ranks},
  journal = {The Journal of Experimental Education},
  volume = {62},
  number = {1},
  pages = {75--86},
  year = {1993},
  publisher = {Routledge},
  doi = {10.1080/00220973.1993.9943832},
  URL = {   
    https://doi.org/10.1080/00220973.1993.9943832
  },
  eprint = { 
    https://doi.org/10.1080/00220973.1993.9943832
  }
}


@article{knn,
  author={Cover, T. and Hart, P.},
  journal={IEEE Transactions on Information Theory}, 
  title={Nearest neighbor pattern classification}, 
  year={1967},
  volume={13},
  number={1},
  pages={21-27},
  keywords={},
  doi={10.1109/TIT.1967.1053964}
}


@article{distance_func_knn,
  author = {Hu, Li-Yu and Huang, Min-Wei and Ke, Shih-Wen and Tsai, Chih-Fong},
  title = {The distance function effect on k-nearest neighbor classification for medical datasets},
  journal = {SpringerPlus},
  year = {2016},
  volume = {5},
  number = {1},
  pages = {1304},
  doi = {10.1186/s40064-016-2941-7},
  url = {https://doi.org/10.1186/s40064-016-2941-7},
  abstract = {K-nearest neighbor (k-NN) classification is conventional non-parametric classifier, which has been used as the baseline classifier in many pattern classification problems. It is based on measuring the distances between the test data and each of the training data to decide the final classification output.},
  issn = {2193-1801},
  date = {2016/08/09},
}

@article{label_ecoding, 
title={An Euclidean Distance Based on Tensor Product Graph Diffusion Related Attribute Value Embedding for Nominal Data Clustering}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11681}, DOI={10.1609/aaai.v32i1.11681}, abstractNote={ &lt;p&gt; Not like numerical data clustering, nominal data clustering is a very difficult problem because there exists no natural relative ordering between nominal attribute values. This paper mainly aims to make the Euclidean distance measure appropriate to nominal data clustering, and the core idea is the attribute value embedding, namely, transforming each nominal attribute value into a numerical vector. This embedding method consists of four steps. In the first step, the weights, which can quantify the amount of information in attribute values, is calculated for each value in each nominal attribute based on each object and its k nearest neighbors. In the second step, an intra-attribute value similarity matrix is created for each nominal attribute by using the attribute value’s weights. In the third step, for each nominal attribute, we find another attribute with the maximal dependence on it, and build an inter-attribute value similarity matrix on the basis of the attribute value’s weights related to these two attributes. In the last step, a diffusion matrix of each nominal attribute is constructed by the tensor product graph diffusion process, and this step can cause the acquired value embedding to contain simultaneously the intra- and inter-attribute value similarities information. To evaluate the effectiveness of our proposed method, experiments are done on 10 data sets. Experimental results demonstrate that our method not only enables the Euclidean distance to be used for nominal data clustering, but also can acquire the better clustering performance than several existing state-of-the-art approaches. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Gu, Lei and Zhou, Ningning and Zhao, Yang}, year={2018}, month={Apr.} }


@article{numpy,
  title={Array programming with NumPy},
  author={Harris, Charles R and Millman, K Jarrod and Van Der Walt, St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J and others},
  journal={Nature},
  volume={585},
  number={7825},
  pages={357--362},
  year={2020},
  publisher={Nature Publishing Group UK London}
}


@article{pandas,
  title={Data structures for statistical computing in Python.},
  author={McKinney, Wes and others},
  journal={Scipy},
  volume={445},
  number={1},
  pages={51--56},
  year={2010}
}


@misc{matplotlib,
  title={Matplotlib: A 2D Graphics Environment. Com-464 puting in Science \& Engineering, 9 (3), 90--95},
  author={Hunter, JD},
  year={2007}
}

@article{seaborn,
  title={Seaborn: statistical data visualization},
  author={Waskom, Michael L},
  journal={Journal of Open Source Software},
  volume={6},
  number={60},
  pages={3021},
  year={2021}
}

@article{scipy,
  title={SciPy 1.0: fundamental algorithms for scientific computing in Python},
  author={Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and others},
  journal={Nature methods},
  volume={17},
  number={3},
  pages={261--272},
  year={2020},
  publisher={Nature Publishing Group}
}


@article{dimensionality,
  title={High-dimensional data analysis: The curses and blessings of dimensionality},
  author={Donoho, David L and others},
  journal={AMS math challenges lecture},
  volume={1},
  number={2000},
  pages={32},
  year={2000}
}


@inproceedings{data_cleaning,
  title={Data cleaning: Overview and emerging challenges},
  author={Chu, Xu and Ilyas, Ihab F and Krishnan, Sanjay and Wang, Jiannan},
  booktitle={Proceedings of the 2016 international conference on management of data},
  pages={2201--2206},
  year={2016}
}

@article{data_preprocess,
  title={Review of data preprocessing techniques in data mining},
  author={Alasadi, Suad A and Bhaya, Wesam S},
  journal={Journal of Engineering and Applied Sciences},
  volume={12},
  number={16},
  pages={4102--4107},
  year={2017}
}

@article{de2018wilcoxon,
  title={Wilcoxon rank sum test drift detector},
  author={de Barros, Roberto Souto Maior and Hidalgo, Juan Isidro Gonz{\'a}lez and de Lima Cabral, Danilo Rafael},
  journal={Neurocomputing},
  volume={275},
  pages={1954--1963},
  year={2018},
  publisher={Elsevier}
}