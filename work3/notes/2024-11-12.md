# 2024-11-12 Notes

## Project is due 2024-12-08

* Unsupervised learning
* Clustering
    * Optics <-> using sklearn, same for everyone
    * Spectral clustering <-> using sklearn, same for everyone
    * K means (using our own code)
    * Improved K means (using our own code)
        * 5 or 6 algorithms, select 2 (the difficulty of the algorithm affects the grade)
    * FCM (using your own code)
    * Perform evaluation (using sklearn metrics)

How we choose parameters for Optics and Spectral clustering affects results significantly

Be careful about the time needed to run experiments! Depending on the parameters, the run time may change considerably

Random partition of clusters requires running several times, since it produces different results each time.

### Packages allowed in this exercise:
* arff_loader
* numpy
* pandas
* scipy
* sklearn (only for some parts)
* matplotlib
* seaborn

### Session 1
* Intro
* Optics (density-based, not in theory)
    * HTTPS://SCIKIT-LEARN.ORG/STABLE/MODULES/GENERATED/SKLEARN.CLUSTER.OPTICS.HTML
* Spectral clustering (graph-based, not in theory)

### Session 2
* K means and improved K means
* Fuzzy clustering

### Session 3
* Validation techniques

## Density-based clustering

* OPTICS is based on DBSCAN
* In density based clustering we partition points into dense regions separated by not-so-dense regions.
* DBSCAN: Preliminary concepts
    * Characterization of points
        * Epsilon parameter, or ℇ
            * Density = number of points within a specified radius (Eps)
            * For any point p, the epsilon defines a distance around the point
        * MinPts parameter (minimum amount of points)
            * How many points must be within the ℇ distance of a point p (including the point) to form a cluster
        * Core points
            * A point is a core point if it has more than a specified number of points (MinPts) within its ℇ distance (including
        itself)
            * These points belong in a dense region and are at the interior of a cluster
        * Border point
            * A border point has fewer than MinPts within ℇ, but is in the neighborhood of a core point
            * Noise point
        * A noise point is any point that is not a core point or a border point
    * Parameter estimation
        * minPts
            * derived from the number of dimensions D in the data set, as **minPts ≥ D + 1**
                * minPts = 1 does not make sense, as then every point on its own will already be a cluster
                * minPts must be chosen at least 3. Larger is better.
                * larger the dataset, the larger the value of minPts should be chosen
        * ℇ
            * value can be chosen by using a k-distance graph
            * If ℇ is chosen much too small, a large part of the data will not be clustered
            * If too high value, majority of objects will be in the same cluster
            * In general, **small values of ℇ are preferable**
* OPTICS: Ordering Points To Identify the Clustering Structure
    * Similar to DBSCAN, but they ordered the points

## Graph-based clustering (e.g. Spectral Clustering)

* clustering based on connectivity
    * Clustering based on the spectrum of the graph
        * the multiset of the eigenvalues of the Laplacian matrix
    * Treats clustering as a graph partitioning problem without making specific assumptions on the form of the clusters
    * Clusters points using eigenvectors of matrices derived from the data
    * Maps data to a low-dimensional space that are separated and can be easily clustered
* Authors disagree:
    * Which eigenvectors to use
    * How to derive clusters from these eigenvectors
* Method 1: Partition using only one eigenvector at a time **<-Probably not for our project**
    * Use procedure recursively
    * Example: Image Segmentation
        * Uses 2nd (smallest) eigenvector to define optimal cut
        * Recursively generates two clusters with each cut
* Method 2: Use k eigenvectors (k chosen by user)
    * Directly compute k-way partitioning
    * Experimentally has been seen to be “better”