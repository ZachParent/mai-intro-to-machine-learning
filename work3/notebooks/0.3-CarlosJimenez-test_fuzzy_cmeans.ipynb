{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import glob\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "\n",
    "# Set up logger\n",
    "logger = logging.getLogger(__name__)\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_DIR = pathlib.Path(os.getcwd()).absolute()\n",
    "DATA_DIR = os.path.join(SCRIPT_DIR.parent, \"data\")\n",
    "PREPROCESSED_DATA_DIR = f'{DATA_DIR}/1_preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FuzzyCMeansParamsGrid = {\n",
    "#     \"n_clusters\": [2, 3, 4],\n",
    "#     \"fuzzyness\": [1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5],\n",
    "#     \"suppression_factor\": [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# }\n",
    "\n",
    "FuzzyCMeansParamsGrid = {\n",
    "    \"n_clusters\": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    \"fuzzyness\": [1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzyCMeans(ClusterMixin, BaseEstimator):\n",
    "    def __init__(self, n_clusters: int, fuzzyness: float, suppression_rule='theta', suppression_param=0.5):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.fuzzyness = fuzzyness\n",
    "        self.suppression_rule = suppression_rule  # 'theta', 'rho', 'beta', 'kappa', 'tau', 'sigma', 'xi'\n",
    "        self.suppression_param = suppression_param\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = X.to_numpy()\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        print(n_samples, n_features)\n",
    "        \n",
    "        # Initialize cluster prototypes (randomly for this example)\n",
    "        print(np.random.choice(n_samples, self.n_clusters, replace=False))\n",
    "        self.cluster_prototypes_ = X[np.random.choice(n_samples, self.n_clusters, replace=False)]\n",
    "\n",
    "        # Initialize fuzzy membership matrix\n",
    "        U = self._initialize_membership(X)\n",
    "\n",
    "        # Main loop (alternating optimization)\n",
    "        while True:\n",
    "            previous_prototypes = np.copy(self.cluster_prototypes_)\n",
    "\n",
    "            # 4. Compute distances\n",
    "            distances = self._compute_distances(X)\n",
    "            # print('distances', distances)\n",
    "\n",
    "            # 5. Update fuzzy membership matrix\n",
    "            U = self._update_membership(distances)\n",
    "            # print('after fuzzyness', U)\n",
    "\n",
    "            # 6. Apply suppression (context-sensitive)\n",
    "            U = self._apply_suppression(U, distances)\n",
    "            # print('after suppression', U)\n",
    "\n",
    "            # 7. Update cluster prototypes\n",
    "            self.cluster_prototypes_ = self._update_prototypes(X, U)\n",
    "\n",
    "            # print('cluster protos', self.cluster_prototypes_)\n",
    "            # print('previous protos', previous_prototypes)\n",
    "\n",
    "            # 8. Check for convergence\n",
    "            diff = np.linalg.norm(self.cluster_prototypes_ - previous_prototypes)\n",
    "            # print('diff',diff)\n",
    "            if diff < 1e-4:\n",
    "                break\n",
    "\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "        self.clusters_ = np.argmax(U, axis=1)\n",
    "        self.centroids_ = self.cluster_prototypes_\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_predict(self, data):\n",
    "        \"\"\"\n",
    "        Fit the model and return cluster labels.\n",
    "        \"\"\"\n",
    "        self.fit(data)\n",
    "        return self.clusters_\n",
    "\n",
    "    def _initialize_membership(self, X):\n",
    "        # print('initialize membership')\n",
    "        U = np.random.rand(len(X), self.n_clusters)\n",
    "        U = U / np.sum(U, axis=1, keepdims=True)  # Normalize to satisfy probabilistic constraint\n",
    "        return U\n",
    "\n",
    "    def _compute_distances(self, X):\n",
    "        # print('compute distances')\n",
    "        distances = np.zeros((len(X), self.n_clusters))\n",
    "        for i in range(self.n_clusters):\n",
    "            # print(self.cluster_prototypes_)\n",
    "            distances[:, i] = np.linalg.norm(X - self.cluster_prototypes_[i], axis=1)\n",
    "        return distances\n",
    "\n",
    "    def _update_membership(self, distances):\n",
    "        # print('update membership')\n",
    "        m = self.fuzzyness\n",
    "        # print('m-1', m-1)\n",
    "        U = np.power(distances, -2 / (m - 1-1e-6))\n",
    "        U = np.where(np.isinf(U), 1.0, U) # handles possible inf values\n",
    "        U = U / np.sum(U, axis=1, keepdims=True)  # Normalize\n",
    "\n",
    "        return U\n",
    "\n",
    "    def _apply_suppression(self, U, distances):\n",
    "        # print('apply suppression')\n",
    "        suppressed_U = np.zeros_like(U)\n",
    "        winner_indices = np.argmax(U, axis=1)\n",
    "\n",
    "        for k in range(len(U)):\n",
    "            w = winner_indices[k]  # Winner cluster index\n",
    "            alpha_k = self._compute_suppression_rate(U[k, w], distances[k], w)\n",
    "\n",
    "            suppressed_U[k, :] = alpha_k * U[k, :]\n",
    "            suppressed_U[k, w] = 1 - alpha_k + alpha_k * U[k, w]\n",
    "\n",
    "        return suppressed_U\n",
    "\n",
    "    def _compute_suppression_rate(self, u_w, distances_k, w):\n",
    "        # print('compute suppression rate')\n",
    "        m = self.fuzzyness\n",
    "        rule = self.suppression_rule\n",
    "        param = self.suppression_param\n",
    "\n",
    "        if rule == 'theta':\n",
    "            return 1 / (1 - u_w + u_w * (1 - param) ** (2 / (1 - m)))\n",
    "        elif rule == 'rho':\n",
    "            return 1 / (1 - u_w + param ** (2 / (1 - m)) * u_w ** ((3 - m) / (1 - m)))\n",
    "        elif rule == 'beta':\n",
    "            return 1 / (1 + u_w * (u_w ** (2 * param / (1 - m) / (1 - param)) - 1))\n",
    "        elif rule == 'kappa':\n",
    "            return 1 / (1 - u_w + u_w * (0.5 - (2 * param - 1) / 2 * np.sin(np.pi * u_w)) ** (2 / (1 - m)))\n",
    "        elif rule == 'tau':\n",
    "            return (1 - param) / (1 + u_w * param)\n",
    "        elif rule == 'sigma':\n",
    "            return (1 - u_w ** param) / (1 - u_w)\n",
    "        elif rule == 'xi':\n",
    "            return (1 - (np.sin(np.pi * u_w / 2)) ** param) / (1 - u_w)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid suppression rule\")\n",
    "\n",
    "    def _update_prototypes(self, X, U):\n",
    "        # print('update prototypes')\n",
    "        m = self.fuzzyness\n",
    "        U_m = np.power(U, m)\n",
    "        new_prototypes = np.dot(U_m.T, X) / np.sum(U_m, axis=0, keepdims=True).T\n",
    "        return new_prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = f\"{DATA_DIR}/1_preprocessed/synthetic.csv\"\n",
    "preprocessed_data  = pd.read_csv(data_path)\n",
    "preprocessed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "features_data = preprocessed_data.iloc[:, :-1]\n",
    "# features_data = features_data.sample(n=5)\n",
    "features_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'synthetic'\n",
    "model_name = 'fuzzy_cmeans'\n",
    "from tools.config import PREPROCESSED_DATA_DIR, CLUSTERED_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_clusters': 2, 'fuzzyness': 1.5}\n",
      "1000 2\n",
      "[294 915]\n",
      "{'n_clusters': 2, 'fuzzyness': 2}\n",
      "1000 2\n",
      "[348 149]\n",
      "{'n_clusters': 2, 'fuzzyness': 2.5}\n",
      "1000 2\n",
      "[  3 409]\n",
      "{'n_clusters': 2, 'fuzzyness': 3}\n",
      "1000 2\n",
      "[346 657]\n",
      "{'n_clusters': 2, 'fuzzyness': 3.5}\n",
      "1000 2\n",
      "[279 256]\n",
      "{'n_clusters': 2, 'fuzzyness': 4}\n",
      "1000 2\n",
      "[715 469]\n"
     ]
    }
   ],
   "source": [
    "params_grid = FuzzyCMeansParamsGrid\n",
    "i = 0\n",
    "for params in product(*params_grid.values()): \n",
    "    param_dict = dict(zip(params_grid.keys(), params))\n",
    "    print(param_dict)\n",
    "    model = FuzzyCMeans(**param_dict)\n",
    "\n",
    "    clustered_data_dir = CLUSTERED_DATA_DIR / dataset / model_name\n",
    "    os.makedirs(clustered_data_dir, exist_ok=True)\n",
    "\n",
    "    clusters = model.fit_predict(features_data)\n",
    "\n",
    "\n",
    "    clustered_data = pd.concat(\n",
    "        [preprocessed_data.iloc[:, :-1], pd.Series(clusters, name=\"cluster\")], axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    clustered_data_path = clustered_data_dir / f\"{','.join(f'{k}={v}' for k, v in param_dict.items())}.csv\"\n",
    "    clustered_data.to_csv(clustered_data_path, index=False)\n",
    "\n",
    "    if i == 5:\n",
    "        break\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
