@article{Friedman_posthoc,
author = {Dulce G. Pereira, Anabela Afonso and Fátima Melo Medeiros},
title = {Overview of Friedman’s Test and Post-hoc Analysis},
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {44},
  number = {10},
  pages = {2636--2653},
  year = {2015},
  publisher = {Taylor \& Francis},
  doi = {10.1080/03610918.2014.931971},
  URL = { 
    https://doi.org/10.1080/03610918.2014.931971
  },
  eprint = {   
    https://doi.org/10.1080/03610918.2014.931971
  }

}

@article{Friedman_anova,
  author = {Donald W. Zimmerman and Bruno D. Zumbo},
  title = {Relative Power of the Wilcoxon Test, the Friedman Test, and Repeated-Measures ANOVA on Ranks},
  journal = {The Journal of Experimental Education},
  volume = {62},
  number = {1},
  pages = {75--86},
  year = {1993},
  publisher = {Routledge},
  doi = {10.1080/00220973.1993.9943832},
  URL = {   
    https://doi.org/10.1080/00220973.1993.9943832
  },
  eprint = { 
    https://doi.org/10.1080/00220973.1993.9943832
  }
}

@article{label_encoding, 
title={An Euclidean Distance Based on Tensor Product Graph Diffusion Related Attribute Value Embedding for Nominal Data Clustering}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11681}, DOI={10.1609/aaai.v32i1.11681}, abstractNote={ &lt;p&gt; Not like numerical data clustering, nominal data clustering is a very difficult problem because there exists no natural relative ordering between nominal attribute values. This paper mainly aims to make the Euclidean distance measure appropriate to nominal data clustering, and the core idea is the attribute value embedding, namely, transforming each nominal attribute value into a numerical vector. This embedding method consists of four steps. In the first step, the weights, which can quantify the amount of information in attribute values, is calculated for each value in each nominal attribute based on each object and its k nearest neighbors. In the second step, an intra-attribute value similarity matrix is created for each nominal attribute by using the attribute value’s weights. In the third step, for each nominal attribute, we find another attribute with the maximal dependence on it, and build an inter-attribute value similarity matrix on the basis of the attribute value’s weights related to these two attributes. In the last step, a diffusion matrix of each nominal attribute is constructed by the tensor product graph diffusion process, and this step can cause the acquired value embedding to contain simultaneously the intra- and inter-attribute value similarities information. To evaluate the effectiveness of our proposed method, experiments are done on 10 data sets. Experimental results demonstrate that our method not only enables the Euclidean distance to be used for nominal data clustering, but also can acquire the better clustering performance than several existing state-of-the-art approaches. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Gu, Lei and Zhou, Ningning and Zhao, Yang}, year={2018}, month={Apr.} }


@article{numpy,
  title={Array programming with NumPy},
  author={Harris, Charles R and Millman, K Jarrod and Van Der Walt, St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J and others},
  journal={Nature},
  volume={585},
  number={7825},
  pages={357--362},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{pandas,
  title={Data structures for statistical computing in Python.},
  author={McKinney, Wes and others},
  journal={Scipy},
  volume={445},
  number={1},
  pages={51--56},
  year={2010}
}

@misc{matplotlib,
  title={Matplotlib: A 2D Graphics Environment. Com-464 puting in Science \& Engineering, 9 (3), 90--95},
  author={Hunter, JD},
  year={2007}
}

@article{seaborn,
  title={Seaborn: statistical data visualization},
  author={Waskom, Michael L},
  journal={Journal of Open Source Software},
  volume={6},
  number={60},
  pages={3021},
  year={2021}
}

@article{scipy,
  title={SciPy 1.0: fundamental algorithms for scientific computing in Python},
  author={Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and others},
  journal={Nature methods},
  volume={17},
  number={3},
  pages={261--272},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{dimensionality,
  title={High-dimensional data analysis: The curses and blessings of dimensionality},
  author={Donoho, David L and others},
  journal={AMS math challenges lecture},
  volume={1},
  number={2000},
  pages={32},
  year={2000}
}

@inproceedings{data_cleaning,
  title={Data cleaning: Overview and emerging challenges},
  author={Chu, Xu and Ilyas, Ihab F and Krishnan, Sanjay and Wang, Jiannan},
  booktitle={Proceedings of the 2016 international conference on management of data},
  pages={2201--2206},
  year={2016}
}

@article{data_preprocess,
  title={Review of data preprocessing techniques in data mining},
  author={Alasadi, Suad A and Bhaya, Wesam S},
  journal={Journal of Engineering and Applied Sciences},
  volume={12},
  number={16},
  pages={4102--4107},
  year={2017}
}

@article{de2018wilcoxon,
  title={Wilcoxon rank sum test drift detector},
  author={de Barros, Roberto Souto Maior and Hidalgo, Juan Isidro Gonz{\'a}lez and de Lima Cabral, Danilo Rafael},
  journal={Neurocomputing},
  volume={275},
  pages={1954--1963},
  year={2018},
  publisher={Elsevier}
}
