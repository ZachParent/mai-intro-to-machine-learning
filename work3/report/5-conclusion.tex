\section{Conclusion}
\label{sec:conclusion}

Clustering is a core machine learning technique used in many applications, but choosing the right clustering algorithm and hyperparameters is not always straightforward. In this report, we have evaluated several clustering algorithms and their hyperparameters on three distinct datasets, exploring the strengths and weaknesses of each method. Here we present our key findings and practical implications of the results.

\subsection{Key Findings}

\begin{itemize}
    \item K-means demonstrated consistent performance with moderate execution times (0.003-0.38s) and balanced clustering metrics
    \item Fuzzy C-means showed particularly fast runtimes even on the largest dataset (mushroom), while offering similar performance to K-means
    \item Global K-means exhibited longer execution times but provided more stable clustering than K-means results across different random initializations
    \item Gmeans did not demonstrate competitive results generally, but exhibited decent F1 scores on the vowel dataset
    \item OPTICS generally showed weak performance, probably due to the lack of natural density variations in the datasets
    \item Spectral clustering performed well with nearest-neighbors affinity, especially on the vowel dataset, achieving the best F1 score on this difficult-to-cluster dataset
\end{itemize}

\subsection{Practical Implications}

\begin{itemize}
    \item For well-separated clusters: K-means offers the best balance of speed and accuracy
    \item For overlapping clusters: Fuzzy C-means provides more nuanced cluster assignments, and it's faster than K-means
    \item For datasets with varying densities and non-convex shapes: OPTICS or Spectral clustering are more suitable
    \item When cluster number is unknown: Gmeans, Fuzzy C-means, OPTICS or Spectral clustering are preferable
    \item When stability is crucial: Global K-means offers more consistent results, at the cost of longer runtimes
\end{itemize}

\subsection{Limitations and Future Work}

\begin{itemize}
    \item Current analysis is limited to specific parameter ranges and could be expanded
    \item Computational efficiency could be improved, particularly for Global K-means and OPTICS
    \item Integration of multiple algorithms in an ensemble approach could potentially yield better results
\end{itemize}

\subsection{Final Remarks}

Each clustering algorithm demonstrates distinct strengths and weaknesses, suggesting that the choice of algorithm should be primarily driven by specific application requirements and data characteristics. While K-means and Fuzzy C-means offer good general-purpose solutions, specialized algorithms like OPTICS and Spectral clustering provide advantages for specific data distributions. Future work should focus on developing hybrid approaches that can combine the strengths of multiple algorithms while mitigating their individual weaknesses.

